## ğŸ“š Part 1: Introduction to Graph Learning
### 5ì¥. ê¸°ë³¸ ì¸ê³µì‹ ê²½ë§ì„ ì‚¬ìš©í•œ ë…¸ë“œ íŠ¹ì„±ê°’ í¬í•¨ì‹œí‚¤ê¸° (Including Node Features with Vanilla Neural Networks)

#### ğŸ“Š 1. ê·¸ë˜í”„ ë°ì´í„°ì…‹ ì†Œê°œ (Introducing graph datasets)

**ì‚¬ìš©í•  ë‘ ë°ì´í„°ì…‹:**
- **Cora ë°ì´í„°ì…‹**
- **Facebook Page-Page ë°ì´í„°ì…‹**

##### ğŸ“š Cora ë°ì´í„°ì…‹
2008ë…„ Sen ë“±ì— ì˜í•´ ì†Œê°œëœ CoraëŠ” ê³¼í•™ ë¬¸í—Œì—ì„œ ë…¸ë“œ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. 2,708ê°œì˜ ë…¼ë¬¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê° ì—°ê²°ì€ ì°¸ì¡°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

**Cora ë°ì´í„°ì…‹ íŠ¹ì§•**
- ë…¸ë“œ: 2,708ê°œì˜ ë…¼ë¬¸
- ì—ì§€: ì¸ìš© ê´€ê³„
- ë…¸ë“œ íŠ¹ì„±: 1,433ê°œ ê³ ìœ  ë‹¨ì–´ì˜ ì´ì§„ ë²¡í„° (0: ë‹¨ì–´ ì—†ìŒ, 1: ë‹¨ì–´ ìˆìŒ)
- ëª©í‘œ: 7ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ê° ë…¸ë“œ ë¶„ë¥˜
- í‘œí˜„: ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì´ì§„ bag of wordsë¼ê³ ë„ ë¶ˆë¦¼

**ì‹œê°í™”**
ê·¸ë˜í”„ê°€ ë„ˆë¬´ ì»¤ì„œ networkx ê°™ì€ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‹œê°í™”í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì „ìš© ë„êµ¬ë“¤ì´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤: **yEd Live** (https://www.yworks.com/yed-live/), **Gephi** (https://gephi.org/)

```
Cora ë°ì´í„°ì…‹ ì‹œê°í™” (yEd Live)
- ì£¼í™©ìƒ‰ ë…¸ë“œ: ë…¼ë¬¸
- ì´ˆë¡ìƒ‰ ì—°ê²°: ì¸ìš© ê´€ê³„
- í´ëŸ¬ìŠ¤í„° í˜•ì„±: ìƒí˜¸ ì—°ê²°ëœ ë…¼ë¬¸ë“¤
```

**PyTorch Geometricìœ¼ë¡œ ë°ì´í„°ì…‹ ë¡œë“œ**

```python
from torch_geometric.datasets import Planetoid

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
dataset = Planetoid(root=".", name="Cora")
data = dataset[0]

# ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
print(f'Dataset: {dataset}')
print('---------------')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of nodes: {data.x.shape[0]}')
print(f'Number of features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')
```

ì¶œë ¥:
```
Dataset: Cora()
---------------
Number of graphs: 1
Number of nodes: 2708
Number of features: 1433
Number of classes: 7
```

**ê·¸ë˜í”„ ì†ì„± í™•ì¸**

```python
print(f'Graph:')
print('------')
print(f'Edges are directed: {data.is_directed()}')
print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')
print(f'Graph has loops: {data.has_self_loops()}')
```

ì¶œë ¥:
```
Graph:
------
Edges are directed: False
Graph has isolated nodes: False
Graph has loops: False
```

##### ğŸ“˜ Facebook Page-Page ë°ì´í„°ì…‹

2019ë…„ Rozemberczki ë“±ì— ì˜í•´ ì†Œê°œëœ ì´ ë°ì´í„°ì…‹ì€ 2017ë…„ 11ì›” Facebook Graph APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

**Facebook Page-Page ë°ì´í„°ì…‹ íŠ¹ì§•**
- ë…¸ë“œ: 22,470ê°œì˜ ê³µì‹ Facebook í˜ì´ì§€
- ì—ì§€: ìƒí˜¸ ì¢‹ì•„ìš” ê´€ê³„
- ë…¸ë“œ íŠ¹ì„±: í˜ì´ì§€ ì†Œìœ ìê°€ ì‘ì„±í•œ í…ìŠ¤íŠ¸ ì„¤ëª…ì—ì„œ ìƒì„±ëœ 128ì°¨ì› ë²¡í„°
- ëª©í‘œ: 4ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ (ì •ì¹˜ì¸, ê¸°ì—…, TV í”„ë¡œê·¸ë¨, ì •ë¶€ê¸°ê´€)

**Coraì™€ì˜ ì£¼ìš” ì°¨ì´ì **
- ë…¸ë“œ ìˆ˜ê°€ í›¨ì”¬ ë§ìŒ (2,708 vs 22,470)
- ë…¸ë“œ íŠ¹ì„± ì°¨ì›ì´ í¬ê²Œ ê°ì†Œ (1,433 â†’ 128)
- ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ê°€ ì ìŒ (7ê°œ â†’ 4ê°œ, ë” ì‰¬ìš´ ì‘ì—…)

**ë°ì´í„°ì…‹ ë¡œë“œ**

```python
from torch_geometric.datasets import FacebookPagePage

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
dataset = FacebookPagePage(root=".")
data = dataset[0]

# ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
print(f'Dataset: {dataset}')
print('-----------------------')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of nodes: {data.x.shape[0]}')
print(f'Number of features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')
```

ì¶œë ¥:
```
Dataset: FacebookPagePage()
-----------------------
Number of graphs: 1
Number of nodes: 22470
Number of features: 128
Number of classes: 4
```

**ê·¸ë˜í”„ ì†ì„± í™•ì¸**

```python
print(f'\nGraph:')
print('------')
print(f'Edges are directed: {data.is_directed()}')
print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')
print(f'Graph has loops: {data.has_self_loops()}')
```

ì¶œë ¥:
```
Graph:
------
Edges are directed: False
Graph has isolated nodes: False
Graph has loops: True
```

**ë§ˆìŠ¤í¬ ìƒì„±**
Facebook Page-Page ë°ì´í„°ì…‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ í›ˆë ¨, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë¯€ë¡œ ì„ì˜ë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# ì„ì˜ë¡œ ë§ˆìŠ¤í¬ ìƒì„±
data.train_mask = range(18000)
data.val_mask = range(18001, 20000)
data.test_mask = range(20001, 22470)
```

---

#### ğŸ§® 2. ê¸°ë³¸ ì‹ ê²½ë§ìœ¼ë¡œ ë…¸ë“œ ë¶„ë¥˜ (Classifying nodes with vanilla neural networks)

Zachary's Karate Clubì™€ ë¹„êµí•˜ì—¬ ì´ ë‘ ë°ì´í„°ì…‹ì€ ìƒˆë¡œìš´ ìœ í˜•ì˜ ì •ë³´ì¸ **ë…¸ë“œ íŠ¹ì„±**ì„ í¬í•¨í•©ë‹ˆë‹¤. ì´ëŠ” ì†Œì…œ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ìš©ìì˜ ë‚˜ì´, ì„±ë³„, ê´€ì‹¬ì‚¬ ë“±ê³¼ ê°™ì€ ë…¸ë“œì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**í‘œ í˜•íƒœ ë°ì´í„°ë¡œ ì·¨ê¸‰**
ë…¸ë“œ íŠ¹ì„±ì€ í‘œ í˜•íƒœ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. data.x(ë…¸ë“œ íŠ¹ì„± í¬í•¨)ì™€ data.y(ê° ë…¸ë“œì˜ í´ë˜ìŠ¤ ë¼ë²¨)ë¥¼ ë³‘í•©í•˜ì—¬ ì¼ë°˜ì ì¸ pandas DataFrameìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import pandas as pd

# Cora ë°ì´í„°ì…‹ì„ DataFrameìœ¼ë¡œ ë³€í™˜
df_x = pd.DataFrame(data.x.numpy())
df_x['label'] = pd.DataFrame(data.y)
```

```
í‘œ í˜•íƒœ í‘œí˜„:
     0    1    ...  1432  label
0    0    0    ...    0      3
1    0    0    ...    0      4
...  ...  ...  ...  ...    ...
2707 0    0    ...    0      3
```

**ì •í™•ë„ í•¨ìˆ˜ ì •ì˜**

```python
def accuracy(y_pred, y_true):
    return torch.sum(y_pred == y_true) / len(y_true)
```

##### ğŸ—ï¸ MLP í´ë˜ìŠ¤ êµ¬í˜„

**í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸**

```python
import torch
from torch.nn import Linear
import torch.nn.functional as F
```

**MLP í´ë˜ìŠ¤ êµ¬ì¡°**

```python
class MLP(torch.nn.Module):
    def __init__(self, dim_in, dim_h, dim_out):
        super().__init__()
        self.linear1 = Linear(dim_in, dim_h)
        self.linear2 = Linear(dim_h, dim_out)
    
    def forward(self, x):
        x = self.linear1(x)
        x = torch.relu(x)
        x = self.linear2(x)
        return F.log_softmax(x, dim=1)
    
    def fit(self, data, epochs):
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.parameters(), 
                                   lr=0.01, weight_decay=5e-4)
        
        self.train()
        for epoch in range(epochs+1):
            optimizer.zero_grad()
            out = self(data.x)
            loss = criterion(out[data.train_mask], 
                           data.y[data.train_mask])
            acc = accuracy(out[data.train_mask].argmax(dim=1), 
                         data.y[data.train_mask])
            loss.backward()
            optimizer.step()
            
            if epoch % 20 == 0:
                val_loss = criterion(out[data.val_mask], 
                                   data.y[data.val_mask])
                val_acc = accuracy(out[data.val_mask].argmax(dim=1), 
                                 data.y[data.val_mask])
                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | '
                      f'Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | '
                      f'Val Acc: {val_acc*100:.2f}%')
    
    def test(self, data):
        self.eval()
        out = self(data.x)
        acc = accuracy(out.argmax(dim=1)[data.test_mask], 
                      data.y[data.test_mask])
        return acc
```

##### ğŸ“ˆ MLP í•™ìŠµ ë° í‰ê°€

**Cora ë°ì´í„°ì…‹ì—ì„œ MLP í›ˆë ¨**

```python
# MLP ëª¨ë¸ ìƒì„±
mlp = MLP(dataset.num_features, 16, dataset.num_classes)
print(mlp)

# ëª¨ë¸ í›ˆë ¨
mlp.fit(data, epochs=100)

# í…ŒìŠ¤íŠ¸ ì •í™•ë„ í‰ê°€
acc = mlp.test(data)
print(f'MLP test accuracy: {acc*100:.2f}%')
```

ì¶œë ¥:
```
MLP(
  (linear1): Linear(in_features=1433, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=7, bias=True)
)

Epoch   0 | Train Loss: 1.954 | Train Acc: 14.29% | Val Loss: 1.93 | Val Acc: 30.80%
Epoch  20 | Train Loss: 0.120 | Train Acc: 100.00% | Val Loss: 1.42 | Val Acc: 49.40%
Epoch  40 | Train Loss: 0.015 | Train Acc: 100.00% | Val Loss: 1.46 | Val Acc: 50.40%
Epoch  60 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.44 | Val Acc: 53.40%
Epoch  80 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.40 | Val Acc: 54.60%
Epoch 100 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.39 | Val Acc: 54.20%

ACCëŠ” ë†’ì•„ì•¼í•˜êµ¬~ LossëŠ” ë‚®ì•„ì•¼í•˜ë‹ˆ~

MLP test accuracy: 52.50%
```

**Facebook Page-Page ë°ì´í„°ì…‹ì—ì„œ MLP í›ˆë ¨**

```python
# Facebook ë°ì´í„°ì…‹ì—ì„œ ë™ì¼í•œ ê³¼ì • ë°˜ë³µ
mlp_facebook = MLP(dataset.num_features, 16, dataset.num_classes)
mlp_facebook.fit(data, epochs=100)
acc_facebook = mlp_facebook.test(data)
print(f'MLP test accuracy: {acc_facebook*100:.2f}%')
```

ì¶œë ¥:
```
Epoch   0 | Train Loss: 1.398 | Train Acc: 23.94% | Val Loss: 1.40 | Val Acc: 24.21%
Epoch  20 | Train Loss: 0.652 | Train Acc: 74.52% | Val Loss: 0.67 | Val Acc: 72.64%
Epoch  40 | Train Loss: 0.577 | Train Acc: 77.07% | Val Loss: 0.61 | Val Acc: 73.84%
Epoch  60 | Train Loss: 0.550 | Train Acc: 78.30% | Val Loss: 0.60 | Val Acc: 75.09%
Epoch  80 | Train Loss: 0.533 | Train Acc: 78.89% | Val Loss: 0.60 | Val Acc: 74.79%
Epoch 100 | Train Loss: 0.520 | Train Acc: 79.49% | Val Loss: 0.61 | Val Acc: 74.94%

MLP test accuracy: 74.52%
```

---

#### ğŸ¯ 3. ê¸°ë³¸ ê·¸ë˜í”„ ì‹ ê²½ë§ìœ¼ë¡œ ë…¸ë“œ ë¶„ë¥˜ (Classifying nodes with vanilla graph neural networks)

ì˜ ì•Œë ¤ì§„ GNN ì•„í‚¤í…ì²˜ë¥¼ ì§ì ‘ ì†Œê°œí•˜ëŠ” ëŒ€ì‹ , GNN ë’¤ì— ìˆëŠ” ì‚¬ê³  ê³¼ì •ì„ ì´í•´í•˜ê¸° ìœ„í•´ ìš°ë¦¬ë§Œì˜ ëª¨ë¸ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

##### ğŸ”„ ê·¸ë˜í”„ ì„ í˜• ì¸µì˜ ê°œë…

**ê¸°ë³¸ ì‹ ê²½ë§ ì¸µ**
ê¸°ë³¸ ì‹ ê²½ë§ ì¸µì€ ì„ í˜• ë³€í™˜ h = xWì— í•´ë‹¹í•˜ë©°, ì—¬ê¸°ì„œ xëŠ” ë…¸ë“œì˜ ì…ë ¥ ë²¡í„°ì´ê³  WëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬ì…ë‹ˆë‹¤.

**ê·¸ë˜í”„ ë§¥ë½ì˜ ì¤‘ìš”ì„±**
ë…¸ë“œ íŠ¹ì„±ë§Œìœ¼ë¡œëŠ” ê·¸ë˜í”„ë¥¼ ì˜ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ì˜ í”½ì…€ì²˜ëŸ¼, ë…¸ë“œì˜ ë§¥ë½ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë…¸ë“œë¥¼ ì´í•´í•˜ë ¤ë©´ ì´ì›ƒì„ ì‚´í´ì•¼ í•©ë‹ˆë‹¤.

**ê·¸ë˜í”„ ì„ í˜• ì¸µ ìˆ˜ì‹**
ë…¸ë“œ iì˜ ì´ì›ƒ ì§‘í•©ì„ N(i)ë¼ê³  í•˜ë©´, ê·¸ë˜í”„ ì„ í˜• ì¸µì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```
h_i = Î£ x_j W^T
     j âˆˆ N(i)
```

**í–‰ë ¬ í˜•íƒœë¡œ ë³€í™˜**
ë” íš¨ìœ¨ì ì¸ í–‰ë ¬ ê³±ì…ˆì„ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì‹œ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```
H = ÃƒX
```

ì—¬ê¸°ì„œ Ãƒ = A + I (ì¸ì ‘ í–‰ë ¬ + ìê¸° ë£¨í”„)

##### ğŸ—ï¸ VanillaGNNLayer êµ¬í˜„

```python
class VanillaGNNLayer(torch.nn.Module):
    def __init__(self, dim_in, dim_out):
        super().__init__()
        self.linear = Linear(dim_in, dim_out, bias=False)
    
    def forward(self, x, adjacency):
        x = self.linear(x)
        x = torch.sparse.mm(adjacency, x)
        return x
```

**ì¸ì ‘ í–‰ë ¬ ì¤€ë¹„**

```python
from torch_geometric.utils import to_dense_adj

# ì—ì§€ ì¸ë±ìŠ¤ë¥¼ ë°€ì§‘ ì¸ì ‘ í–‰ë ¬ë¡œ ë³€í™˜
adjacency = to_dense_adj(data.edge_index)[0]
# ìê¸° ë£¨í”„ ì¶”ê°€
adjacency += torch.eye(len(adjacency))
```

##### ğŸ—ï¸ VanillaGNN êµ¬í˜„

```python
class VanillaGNN(torch.nn.Module):
    def __init__(self, dim_in, dim_h, dim_out):
        super().__init__()
        self.gnn1 = VanillaGNNLayer(dim_in, dim_h)
        self.gnn2 = VanillaGNNLayer(dim_h, dim_out)
    
    def forward(self, x, adjacency):
        h = self.gnn1(x, adjacency)
        h = torch.relu(h)
        h = self.gnn2(h, adjacency)
        return F.log_softmax(h, dim=1)
    
    def fit(self, data, epochs):
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.parameters(), 
                                   lr=0.01, weight_decay=5e-4)
        
        self.train()
        for epoch in range(epochs+1):
            optimizer.zero_grad()
            out = self(data.x, adjacency)
            loss = criterion(out[data.train_mask], 
                           data.y[data.train_mask])
            acc = accuracy(out[data.train_mask].argmax(dim=1), 
                         data.y[data.train_mask])
            loss.backward()
            optimizer.step()
            
            if epoch % 20 == 0:
                val_loss = criterion(out[data.val_mask], 
                                   data.y[data.val_mask])
                val_acc = accuracy(out[data.val_mask].argmax(dim=1), 
                                 data.y[data.val_mask])
                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | '
                      f'Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | '
                      f'Val Acc: {val_acc*100:.2f}%')
    
    def test(self, data):
        self.eval()
        out = self(data.x, adjacency)
        acc = accuracy(out.argmax(dim=1)[data.test_mask], 
                      data.y[data.test_mask])
        return acc
```

##### ğŸ“ˆ VanillaGNN í•™ìŠµ ë° í‰ê°€

**Cora ë°ì´í„°ì…‹ì—ì„œ GNN í›ˆë ¨**

```python
# GNN ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
gnn = VanillaGNN(dataset.num_features, 16, dataset.num_classes)
print(gnn)
gnn.fit(data, epochs=100)
acc = gnn.test(data)
print(f'\nGNN test accuracy: {acc*100:.2f}%')
```

ì¶œë ¥:
```
VanillaGNN(
  (gnn1): VanillaGNNLayer(
    (linear): Linear(in_features=1433, out_features=16, bias=False)
  )
  (gnn2): VanillaGNNLayer(
    (linear): Linear(in_features=16, out_features=7, bias=False)
  )
)

Epoch   0 | Train Loss: 2.008 | Train Acc: 20.00% | Val Loss: 1.96 | Val Acc: 23.40%
Epoch  20 | Train Loss: 0.047 | Train Acc: 100.00% | Val Loss: 2.04 | Val Acc: 74.60%
Epoch  40 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 2.49 | Val Acc: 75.20%
Epoch  60 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 2.61 | Val Acc: 74.60%
Epoch  80 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 2.61 | Val Acc: 75.20%
Epoch 100 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 2.56 | Val Acc: 75.00%

GNN test accuracy: 76.80%
```

#### ğŸ† ì„±ëŠ¥ ë¹„êµ ë° ê²°ê³¼ ë¶„ì„

**100íšŒ ë°˜ë³µ ì‹¤í—˜ ê²°ê³¼**

| ë°ì´í„°ì…‹ | MLP | GNN |
|----------|-----|-----|
| **Cora** | 53.47% (Â±1.81%) | 74.98% (Â±1.50%) |
| **Facebook** | 75.21% (Â±0.40%) | 84.85% (Â±1.68%) |

**ê²°ê³¼ ë¶„ì„**

MLPëŠ” Coraì—ì„œ ë‚®ì€ ì •í™•ë„ë¥¼ ë³´ì´ì§€ë§Œ, Facebook Page-Page ë°ì´í„°ì…‹ì—ì„œëŠ” ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‘ ê²½ìš° ëª¨ë‘ ìš°ë¦¬ì˜ ê¸°ë³¸ GNNì— ì˜í•´ ëŠ¥ê°€ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ë…¸ë“œ íŠ¹ì„±ì— í† í´ë¡œì§€ ì •ë³´ë¥¼ í¬í•¨ì‹œí‚¤ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

**GNNì˜ ìš°ìˆ˜ì„±**
- í‘œ í˜•íƒœ ë°ì´í„° ëŒ€ì‹  GNNì€ ê° ë…¸ë“œì˜ ì „ì²´ ì´ì›ƒì„ ê³ ë ¤
- ì´ ì˜ˆì œì—ì„œ 10-20%ì˜ ì •í™•ë„ í–¥ìƒ
- ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©

**GNNì˜ ì‹¤ìš©ì  ì ìš©**
- ì‹ ì•½ ê°œë°œ: ìƒˆë¡œìš´ í•­ìƒì œ ë°œê²¬
- ì¶”ì²œ ì‹œìŠ¤í…œ: ì‚¬ìš©ì-ìƒí’ˆ ê´€ê³„ ë¶„ì„
- êµí†µ ì˜ˆì¸¡: ë‹¤ì–‘í•œ ê²½ë¡œì™€ êµí†µ ìˆ˜ë‹¨ ê´€ê³„ ê³ ë ¤
