# VanillaGNN êµ¬ì¡° ë¶„ì„

## ğŸ“‹ ëª©ì°¨
1. [ëª¨ë¸ ê°œìš”](#-1-ëª¨ë¸-ê°œìš”)
2. [ì…ë ¥ ë°ì´í„° êµ¬ì¡°](#-2-ì…ë ¥-ë°ì´í„°-êµ¬ì¡°)
3. [ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜](#-3-ë„¤íŠ¸ì›Œí¬-ì•„í‚¤í…ì²˜)
4. [Forward ì—°ì‚° ê³¼ì •](#-4-forward-ì—°ì‚°-ê³¼ì •)
5. [VanillaGNNLayer ìƒì„¸ ë¶„ì„](#-5-vanillagnnlayer-ìƒì„¸-ë¶„ì„)
6. [í•™ìŠµ ê³¼ì •](#-6-í•™ìŠµ-ê³¼ì •)
7. [ì‹¤í—˜ ê²°ê³¼ ë° ë¶„ì„](#-7-ì‹¤í—˜-ê²°ê³¼-ë°-ë¶„ì„)
8. [MLP vs GNN ë¹„êµ](#-8-mlp-vs-gnn-ë¹„êµ)
9. [ì˜µí‹°ë§ˆì´ì € ì •ë¦¬](#-9-ì˜µí‹°ë§ˆì´ì €-ì •ë¦¬)

---

## âœ… 1. ëª¨ë¸ ê°œìš”

```python
class VanillaGNN(torch.nn.Module):
```

ì´ í´ë˜ìŠ¤ëŠ” PyTorchì˜ ê¸°ë³¸ ì‹ ê²½ë§ ëª¨ë“ˆì¸ `torch.nn.Module`ì„ ìƒì†í•œ **2-layer GNN**ì…ë‹ˆë‹¤. 

### ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´
**VanillaGNNì€ ê¸°ë³¸ì ìœ¼ë¡œ Dense Layerì— Adjacency Layerë¥¼ ì¶”ê°€í•œ êµ¬ì¡°**ì…ë‹ˆë‹¤.

```python
VanillaGNN (
  (gnn1): VanillaGNNLayer(
    (linear): Linear(in_features=1433, out_features=16, bias=False)
  )
  (gnn2): VanillaGNNLayer(
    (linear): Linear(in_features=16, out_features=7, bias=False)
  )
)
```

### ğŸ—ï¸ ì „ì²´ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TD
    A[ì…ë ¥: X, A] --> B[GNN Layer 1]
    B --> C[ReLU Activation]
    C --> D[GNN Layer 2]
    D --> E[Log Softmax]
    E --> F[ì¶œë ¥: ë…¸ë“œë³„ í´ë˜ìŠ¤ í™•ë¥ ]
    
    subgraph "ì…ë ¥ ë°ì´í„°"
        A1[X: ë…¸ë“œ íŠ¹ì§• í–‰ë ¬<br/>2798 Ã— 1433] 
        A2[A: ì¸ì ‘í–‰ë ¬<br/>2798 Ã— 2798]
    end
    
    subgraph "GNN Layer 1"
        B1[A @ X<br/>2798 Ã— 1433]
        B2[Ã— W1<br/>1433 Ã— 16]
        B3[ì¶œë ¥: 2798 Ã— 16]
    end
    
    subgraph "GNN Layer 2"
        D1[A @ H1<br/>2798 Ã— 16]
        D2[Ã— W2<br/>16 Ã— 7]
        D3[ì¶œë ¥: 2798 Ã— 7]
    end
```

---

## âœ… 2. ì…ë ¥ ë°ì´í„° êµ¬ì¡°

### ğŸ“Š ë°ì´í„° ì°¨ì› ì •ë³´
- **ë…¸ë“œ ìˆ˜**: 2,798ê°œ
- **ì…ë ¥ íŠ¹ì§• ì°¨ì›**: 1,433ê°œ
- **ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜**: 7ê°œ

### ğŸ”¢ í–‰ë ¬ ì°¨ì› ì •ë¦¬

| êµ¬ì„± ìš”ì†Œ | ì°¨ì› | ì„¤ëª… |
|---------|------|------|
| **X (ë…¸ë“œ íŠ¹ì§•)** | 2798 Ã— 1433 | ê° ë…¸ë“œì˜ 1433ê°œ íŠ¹ì§• |
| **A (ì¸ì ‘í–‰ë ¬)** | 2798 Ã— 2798 | ê·¸ë˜í”„ ì—°ê²° ì •ë³´ |
| **W1 (ê°€ì¤‘ì¹˜1)** | 1433 Ã— 16 | ì…ë ¥ â†’ ì€ë‹‰ì¸µ |
| **W2 (ê°€ì¤‘ì¹˜2)** | 16 Ã— 7 | ì€ë‹‰ì¸µ â†’ ì¶œë ¥ì¸µ |

### ğŸ¯ í•µì‹¬ ìˆ˜ì‹: `A^T Ã— W`
- **A^T**: ì¸ì ‘í–‰ë ¬ì˜ ì „ì¹˜ (2798 Ã— 2798)
- **W**: ê°€ì¤‘ì¹˜ í–‰ë ¬ (1433 Ã— 16)
- **ì—°ì‚°**: `A^T @ X @ W` í˜•íƒœë¡œ ìˆ˜í–‰

---

## âœ… 3. ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜

```python
self.gnn1 = VanillaGNNLayer(dim_in, dim_h)    # 1433 â†’ 16
self.gnn2 = VanillaGNNLayer(dim_h, dim_out)   # 16 â†’ 7
```

### ğŸ›ï¸ ë ˆì´ì–´ êµ¬ì„± ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph LR
    subgraph "ì…ë ¥ì¸µ"
        X[ë…¸ë“œ íŠ¹ì§•<br/>2798 Ã— 1433]
        A[ì¸ì ‘í–‰ë ¬<br/>2798 Ã— 2798]
    end
    
    subgraph "ì€ë‹‰ì¸µ"
        H1[GNN Layer 1<br/>2798 Ã— 16]
        R[ReLU]
    end
    
    subgraph "ì¶œë ¥ì¸µ"
        H2[GNN Layer 2<br/>2798 Ã— 7]
        S[Log Softmax]
    end
    
    X --> H1
    A --> H1
    H1 --> R
    R --> H2
    A --> H2
    H2 --> S
```

---

## âœ… 4. Forward ì—°ì‚° ê³¼ì •

```python
def forward(self, x, adjacency):
    h = self.gnn1(x, adjacency)     # 1ë‹¨ê³„: ì¸ì ‘í–‰ë ¬ê³¼ íŠ¹ì§•ì„ ê³±í•¨
    h = torch.relu(h)               # ë¹„ì„ í˜• í™œì„±í™”
    h = self.gnn2(h, adjacency)     # 2ë‹¨ê³„: ë‹¤ì‹œ ë©”ì‹œì§€ ì „ë‹¬
    return F.log_softmax(h, dim=1)  # ìµœì¢… ì¶œë ¥: ë…¸ë“œ ë¶„ë¥˜ë¥¼ ìœ„í•œ log_softmax
```

### ğŸ”„ ì—°ì‚° íë¦„ ìƒì„¸

```mermaid
graph TD
    A[X: 2798Ã—1433] --> D[GNN Layer 1]
    B[A: 2798Ã—2798] --> D
    C[W1: 1433Ã—16] --> D
    D --> E[H1: 2798Ã—16]
    E --> F[ReLU]
    F --> G[H1_activated: 2798Ã—16]
    G --> H[GNN Layer 2]
    B --> H
    I[W2: 16Ã—7] --> H
    H --> J[H2: 2798Ã—7]
    J --> K[Log Softmax]
    K --> L[ì¶œë ¥: 2798Ã—7]
```

### ğŸ“ ìˆ˜ì‹ í‘œí˜„

**Layer 1**: `Hâ‚ = ReLU(A Ã— X Ã— Wâ‚ + bâ‚)`
- `A Ã— X`: 2798Ã—2798 Ã— 2798Ã—1433 = 2798Ã—1433
- `Ã— Wâ‚`: 2798Ã—1433 Ã— 1433Ã—16 = 2798Ã—16

**Layer 2**: `Hâ‚‚ = A Ã— Hâ‚ Ã— Wâ‚‚ + bâ‚‚`
- `A Ã— Hâ‚`: 2798Ã—2798 Ã— 2798Ã—16 = 2798Ã—16
- `Ã— Wâ‚‚`: 2798Ã—16 Ã— 16Ã—7 = 2798Ã—7

**ìµœì¢… ì¶œë ¥**: `Z = log_softmax(Hâ‚‚)`

---

## âœ… 5. VanillaGNNLayer ìƒì„¸ ë¶„ì„

### ğŸ§© Layer êµ¬ì¡°

```python
class VanillaGNNLayer(torch.nn.Module):
    def __init__(self, dim_in, dim_out):
        super().__init__()
        self.linear = torch.nn.Linear(dim_in, dim_out, bias=False)  # bias=False ì£¼ëª©!

    def forward(self, x, adjacency):
        out = torch.matmul(adjacency, x)        # A @ X
        out = self.linear(out)                  # (A @ X) @ W
        return out
```

### ğŸ” ë©”ì‹œì§€ ì „ë‹¬ ê³¼ì •

```mermaid
graph LR
    subgraph "Aggregation"
        A[ì¸ì ‘í–‰ë ¬ A<br/>2798Ã—2798] 
        X[ë…¸ë“œ íŠ¹ì§• X<br/>2798Ã—1433]
        A --> M[A @ X<br/>ì´ì›ƒ ì •ë³´ ì§‘ê³„]
        X --> M
    end
    
    subgraph "Transformation"
        M --> T[Ã— W<br/>ì„ í˜• ë³€í™˜]
        T --> O[ì¶œë ¥<br/>2798Ã—16]
    end
```

### ğŸ’¡ í•µì‹¬ ì•„ì´ë””ì–´

> **ì´ì›ƒ ë…¸ë“œì˜ ì •ë³´ë¥¼ ëª¨ì•„ì„œ(aggregate) Wë¡œ íˆ¬ì‚¬í•œë‹¤.**

- **Aggregation**: `A @ X` - ê° ë…¸ë“œê°€ ì´ì›ƒ ë…¸ë“œë“¤ì˜ íŠ¹ì§•ì„ í‰ê· /í•©ì‚°
- **Transformation**: `@ W` - ì§‘ê³„ëœ ì •ë³´ë¥¼ ìƒˆë¡œìš´ íŠ¹ì§• ê³µê°„ìœ¼ë¡œ íˆ¬ì‚¬

---

## âœ… 6. í•™ìŠµ ê³¼ì •

### ğŸ¯ ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €

```python
def fit(self, data, epochs):
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)
```

### ğŸ”„ í•™ìŠµ ë£¨í”„

```python
for epoch in range(epochs+1):
    self.train()
    optimizer.zero_grad()
    out = self(data.x, adjacency)  # forward pass
    loss = criterion(out[data.train_mask], data.y[data.train_mask])
    loss.backward()                # backward pass
    optimizer.step()               # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
```

### ğŸ“Š í•™ìŠµ ê³¼ì • ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TD
    A[ì…ë ¥ ë°ì´í„°] --> B[Forward Pass]
    B --> C[ì˜ˆì¸¡ê°’ ê³„ì‚°]
    C --> D[ì†ì‹¤ ê³„ì‚°]
    D --> E[Backward Pass]
    E --> F[ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°]
    F --> G[ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸]
    G --> H{ì—í¬í¬ ì™„ë£Œ?}
    H -->|No| B
    H -->|Yes| I[í…ŒìŠ¤íŠ¸]
```

### ğŸ§ª í…ŒìŠ¤íŠ¸ í•¨ìˆ˜

```python
def test(self, data):
    self.eval()
    out = self(data.x, adjacency)
    acc = accuracy(out[data.test_mask], data.y[data.test_mask])
    return acc
```

---

## âœ… 7. ì‹¤í—˜ ê²°ê³¼ ë° ë¶„ì„

### ğŸ“ˆ í•™ìŠµ ê²°ê³¼

| Epoch | Train Loss | Train Acc | Val Loss | Val Acc |
|-------|------------|-----------|----------|---------|
| 0     | 1.991      | 15.71%    | 2.11     | 9.40%   |
| 20    | 0.065      | 99.29%    | 1.47     | 76.80%  |
| 40    | 0.014      | 100.00%   | 2.11     | 75.40%  |
| 60    | 0.007      | 100.00%   | 2.22     | 75.40%  |
| 80    | 0.004      | 100.00%   | 2.20     | 76.80%  |
| 100   | 0.003      | 100.00%   | 2.19     | 77.00%  |

**ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: 76.60%**

### ğŸš¨ ê³¼ì í•©(Overfitting) ë¶„ì„

```mermaid
graph LR
    subgraph "ê³¼ì í•© íŒ¨í„´"
        A[Train Loss: ì§€ì†ì  ê°ì†Œ<br/>0.003ê¹Œì§€] 
        B[Train Acc: 100% ë‹¬ì„±]
        C[Val Loss: ì¦ê°€ ì¶”ì„¸<br/>2.19ê¹Œì§€]
        D[Val Acc: ì •ì²´<br/>77% ìˆ˜ì¤€]
    end
```

#### ğŸ” ê³¼ì í•© ì¦ìƒ
1. **Train Loss**: 1.991 â†’ 0.003 (ì§€ì†ì  ê°ì†Œ)
2. **Train Accuracy**: 15.71% â†’ 100% (ì™„ë²½í•œ í•™ìŠµ)
3. **Validation Loss**: 2.11 â†’ 2.19 (ì¦ê°€ ì¶”ì„¸)
4. **Validation Accuracy**: 9.40% â†’ 77% (ì •ì²´)

#### ğŸ’¡ ê³¼ì í•© ì›ì¸
- **ëª¨ë¸ ë³µì¡ë„**: 2-layer GNNì´ ë°ì´í„°ì— ë¹„í•´ ë³µì¡
- **ë°ì´í„° ë¶€ì¡±**: í•™ìŠµ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ
- **ì •ê·œí™” ë¶€ì¡±**: Dropoutì´ë‚˜ ë” ê°•í•œ weight decay í•„ìš”

#### ğŸ› ï¸ ê°œì„  ë°©ì•ˆ
1. **Early Stopping**: Validation Loss ì¦ê°€ ì‹œì ì—ì„œ í•™ìŠµ ì¤‘ë‹¨
2. **Dropout ì¶”ê°€**: ê³¼ì í•© ë°©ì§€
3. **Weight Decay ì¦ê°€**: 5e-4 â†’ 1e-3
4. **ëª¨ë¸ ë‹¨ìˆœí™”**: ì€ë‹‰ì¸µ ì°¨ì› ì¶•ì†Œ

---

## âœ… 8. MLP vs GNN ë¹„êµ

| í•­ëª©      | MLP                  | GNN                        |
| ------- | -------------------- | -------------------------- |
| ì—°ê²°ì„±     | ë…¸ë“œ ê°„ ì •ë³´ ê³µìœ  ì—†ìŒ        | ì´ì›ƒ ë…¸ë“œì™€ ì •ë³´ ê³µìœ  (ë©”ì‹œì§€ ì „ë‹¬)      |
| ì…ë ¥ êµ¬ì¡°   | Dense feature matrix | Feature + adjacency matrix |
| ì—°ì‚°      | `x @ W`              | `A @ x @ W`                |
| íŒŒë¼ë¯¸í„° ê³µìœ  | ì—†ìŒ                   | ìˆìŒ (WëŠ” ì—¬ëŸ¬ ë…¸ë“œì— ê³µìœ ë¨)         |

### ğŸ” í•µì‹¬ ì°¨ì´ì  ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "MLP"
        M1[ë…¸ë“œ 1] --> M2[ê°œë³„ ì²˜ë¦¬]
        M3[ë…¸ë“œ 2] --> M4[ê°œë³„ ì²˜ë¦¬]
        M5[ë…¸ë“œ 3] --> M6[ê°œë³„ ì²˜ë¦¬]
    end
    
    subgraph "GNN"
        G1[ë…¸ë“œ 1] --> G2[ì´ì›ƒ ì •ë³´ ì§‘ê³„]
        G3[ë…¸ë“œ 2] --> G2
        G4[ë…¸ë“œ 3] --> G2
        G2 --> G5[ê³µìœ  ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜]
    end
```

---

## âœ… 9. ì˜µí‹°ë§ˆì´ì € ì •ë¦¬

### ğŸ¯ ì£¼ìš” ì˜µí‹°ë§ˆì´ì € ë¹„êµ

| ì˜µí‹°ë§ˆì´ì €     | íŠ¹ì§•                             |
| --------- | ------------------------------ |
| GD        | ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©, ê³„ì‚°ëŸ‰ ë§ìŒ             |
| SGD       | ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸                |
| Momentum  | ì§„ë™ ê°ì†Œ, ì§€ì—­ ìµœì†Œ íƒˆì¶œ ë„ì›€             |
| NAG       | ë¯¸ë˜ ì˜ˆì¸¡ìœ¼ë¡œ ë” ì •êµí•œ ì—…ë°ì´íŠ¸             |
| Adagrad   | íŒŒë¼ë¯¸í„°ë³„ ì ì‘ì  í•™ìŠµë¥ , ê·¸ëŸ¬ë‚˜ ê°ì†Œ ê³¼ë‹¤       |
| RMSprop   | ìµœê·¼ ì—…ë°ì´íŠ¸ ì¤‘ì‹¬, ì•ˆì •ì  í•™ìŠµ ìœ ì§€          |
| **Adam**  | **Momentum + RMSprop + ë³´ì •, ë„ë¦¬ ì‚¬ìš©** |
| Adabelief | Adam ë³€í˜• (ê°„ë‹¨ ì–¸ê¸‰ë§Œ)               |

### ğŸ† VanillaGNNì—ì„œ ì‚¬ìš©í•˜ëŠ” Adam

```python
optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)
```

- **í•™ìŠµë¥ **: 0.01
- **Weight Decay**: 5e-4 (L2 ì •ê·œí™”)
- **ì¥ì **: ìë™ìœ¼ë¡œ í•™ìŠµë¥  ì¡°ì •, ë¹ ë¥¸ ìˆ˜ë ´, ê³¼ì í•© ë°©ì§€

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Optimizer ì¢…ë¥˜ ë° ì •ë¦¬](https://velog.io/@chang0517/Optimizer-%EC%A2%85%EB%A5%98-%EB%B0%8F-%EC%A0%95%EB%A6%AC)

