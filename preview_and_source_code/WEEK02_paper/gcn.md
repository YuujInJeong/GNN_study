# ğŸ“˜ Semi-Supervised Classification with Graph Convolutional Networks (GCN)
---

## 1. ì—°êµ¬ ë¬¸ì œ ì •ì˜ì™€ ë™ê¸° (Problem & Motivation)

### í•µì‹¬ ë¬¸ì œ
- **ê·¸ë˜í”„ êµ¬ì¡° ë°ì´í„°ì—ì„œ semi-supervised ë…¸ë“œ ë¶„ë¥˜ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë¬¸ì œ**
- ê¸°ì¡´ spectral graph convolutionì˜ ê³ ë¹„ìš© ì—°ì‚° ë¬¸ì œ í•´ê²°
- ê·¸ë˜í”„ êµ¬ì¡°ì™€ ë…¸ë“œ featureë¥¼ ë™ì‹œì— í™œìš©í•˜ëŠ” end-to-end í•™ìŠµ ë°©ë²• ì œì•ˆ

### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì 
1. **Spectral Graph Convolution**: 
   - ê³ ìœ ë¶„í•´(eigendecomposition) ê¸°ë°˜ìœ¼ë¡œ $O(N^3)$ ê³„ì‚° ë³µì¡ë„
   - ê·¸ë˜í”„ êµ¬ì¡°ê°€ ê³ ì •ë˜ì–´ ìˆì–´ inductive settingì—ì„œ ì ìš© ë¶ˆê°€
   - ì „ì²´ ê·¸ë˜í”„ì˜ Laplacian matrix í•„ìš”

2. **Spatial Methods**:
   - ì´ì›ƒ ë…¸ë“œì˜ ê°€ì¤‘ì¹˜ê°€ ê³ ì •ë˜ì–´ ìˆìŒ
   - ë…¸ë“œ ì°¨ìˆ˜ì— ë”°ë¥¸ ì •ê·œí™” ë¬¸ì œ
   - í‘œí˜„ë ¥ ì œí•œ

3. **Semi-supervised Learning**:
   - ê·¸ë˜í”„ êµ¬ì¡° ì •ë³´ í™œìš© ë¶€ì¡±
   - ë…¸ë“œ ê°„ ê´€ê³„ ì •ë³´ ì†ì‹¤

### GCNì˜ í•´ê²° ë°©í–¥
- **Chebyshev ë‹¤í•­ì‹ ê·¼ì‚¬**ë¡œ ê³„ì‚° ë³µì¡ë„ ë‹¨ìˆœí™”
- **Renormalization trick**ìœ¼ë¡œ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´
- **Layer-wise propagation rule**ë¡œ end-to-end í•™ìŠµ ê°€ëŠ¥
- **Semi-supervised setting**ì—ì„œ ë¼ë²¨ëœ ë…¸ë“œ ì •ë³´ ì „íŒŒ

---

## 2. ì´ë¡ ì  ë°°ê²½ ë° í•µì‹¬ ê°œë… (Background & Definitions)

### ê·¸ë˜í”„ ì •ì˜
- **ê·¸ë˜í”„**: $G = (V, E)$
  - $V$: ë…¸ë“œ ì§‘í•©, $|V| = N$
  - $E$: ì—£ì§€ ì§‘í•©
- **Adjacency Matrix**: $A \in \mathbb{R}^{N \times N}$
- **Degree Matrix**: $D_{ii} = \sum_j A_{ij}$
- **Laplacian Matrix**: $L = D - A$
- **Normalized Laplacian**: $\tilde{L} = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$

### Spectral Graph Theory
- **Graph Fourier Transform**: $f \rightarrow \hat{f} = U^T f$
- **Inverse Graph Fourier Transform**: $\hat{f} \rightarrow f = U\hat{f}$
- **Spectral Convolution**: $g_\theta * x = Ug_\theta U^Tx$

### Chebyshev Polynomial
- **Chebyshev ë‹¤í•­ì‹**: $T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$
- **ê·¼ì‚¬**: $g_\theta(\Lambda) \approx \sum_{k=0}^K \theta_k T_k(\tilde{\Lambda})$
- **K=1 ê·¼ì‚¬**: $g_\theta * x \approx \theta_0 x + \theta_1 \tilde{L}x$

### í•µì‹¬ ê°€ì •
- **1-hop ì´ì›ƒë§Œ ê³ ë ¤**: $K=1$ Chebyshev ê·¼ì‚¬
- **Self-loop ì¶”ê°€**: $\tilde{A} = A + I_N$
- **Symmetric normalization**: $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$

---

## 3. ëª¨ë¸ êµ¬ì¡° ë° í•µì‹¬ ìˆ˜ì‹ íë¦„ (Model Architecture & Equations)

### ì „ì²´ ëª¨ë¸ êµ¬ì¡°
```
ì…ë ¥ ë…¸ë“œ feature â†’ Graph Convolution Layer â†’ ë¹„ì„ í˜• í™œì„±í™” â†’ Graph Convolution Layer â†’ Softmax â†’ ì¶œë ¥
```

### Graph Convolution Layer ìˆ˜ì‹ íë¦„

#### 3.1 ê¸°ë³¸ Spectral Convolution
$$g_\theta * x = Ug_\theta(\Lambda)U^Tx$$

**ë¬¸ì œì **: ê³ ìœ ë¶„í•´ $O(N^3)$ ê³„ì‚° ë³µì¡ë„

#### 3.2 Chebyshev ë‹¤í•­ì‹ ê·¼ì‚¬
$$g_\theta * x \approx \sum_{k=0}^K \theta_k T_k(\tilde{L})x$$

**íŠ¹ì„±**: 
- $\tilde{L} = \frac{2}{\lambda_{max}}L - I_N$ (ì •ê·œí™”)
- $T_k(\tilde{L}) = 2\tilde{L}T_{k-1}(\tilde{L}) - T_{k-2}(\tilde{L})$
- $K=1$ ê·¼ì‚¬: $g_\theta * x \approx \theta_0 x + \theta_1 \tilde{L}x$

#### 3.3 K=1 ê·¼ì‚¬ ë° ë‹¨ìˆœí™”
$$g_\theta * x \approx \theta_0 x + \theta_1 \tilde{L}x = \theta_0 x + \theta_1(I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x$$

**ë‹¨ìˆœí™”**: $\theta_0 = -\theta_1 = \theta$ë¡œ ì„¤ì •
$$g_\theta * x \approx \theta(I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x$$

#### 3.4 Renormalization Trick
**ë¬¸ì œ**: $I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$ì˜ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±

**í•´ê²°**: $\tilde{A} = A + I_N$, $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$

**ìµœì¢… ìˆ˜ì‹**:
$$H^{(l+1)} = \sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\right)$$

### Layer-wise Propagation Rule

#### 3.5 ë‹¨ì¼ ë ˆì´ì–´ ìˆ˜ì‹
$$H^{(l+1)} = \sigma\left(\hat{A}H^{(l)}W^{(l)}\right)$$

**êµ¬ì„± ìš”ì†Œ**:
- $\hat{A} = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$: ì •ê·œí™”ëœ ì¸ì ‘ í–‰ë ¬
- $H^{(l)} \in \mathbb{R}^{N \times d_l}$: $l$ë²ˆì§¸ ë ˆì´ì–´ì˜ ë…¸ë“œ feature
- $W^{(l)} \in \mathbb{R}^{d_l \times d_{l+1}}$: í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬
- $\sigma(\cdot)$: ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ (ReLU)

#### 3.6 2-layer GCN êµ¬ì¡°
$$Z = f(X, A) = \text{softmax}\left(\hat{A} \text{ReLU}\left(\hat{A}XW^{(0)}\right)W^{(1)}\right)$$

**ì…ë ¥**: $X \in \mathbb{R}^{N \times C}$ (ë…¸ë“œ feature)
**ì¶œë ¥**: $Z \in \mathbb{R}^{N \times F}$ (ë…¸ë“œ í´ë˜ìŠ¤ í™•ë¥ )

---

## 4. í•µì‹¬ ìˆ˜ì‹ì˜ ì˜ˆì‹œì™€ í•´ì„ (Worked Example)

### ì˜ˆì‹œ ê·¸ë˜í”„ ì„¤ì •
```
ë…¸ë“œ: A, B, C (3ê°œ)
ì—£ì§€: A-B, B-C
Feature ì°¨ì›: C=2, Hidden=3, Output=2
```

### ì…ë ¥ ë°ì´í„°
$$X = \begin{bmatrix} 1 & 2 \\ 3 & 1 \\ 0 & 4 \end{bmatrix}, \quad A = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix}$$

### ë‹¨ê³„ë³„ ê³„ì‚°

#### 4.1 Self-loop ì¶”ê°€
$$\tilde{A} = A + I_3 = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{bmatrix}$$

#### 4.2 Degree matrix ê³„ì‚°
$$\tilde{D} = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 2 \end{bmatrix}$$

#### 4.3 ì •ê·œí™”ëœ ì¸ì ‘ í–‰ë ¬
$$\hat{A} = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} = \begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & 0 \\ \frac{1}{\sqrt{6}} & \frac{1}{3} & \frac{1}{\sqrt{6}} \\ 0 & \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \end{bmatrix}$$

#### 4.4 ê°€ì¤‘ì¹˜ í–‰ë ¬ (ì˜ˆì‹œ)
$$W^{(0)} = \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \end{bmatrix}, \quad W^{(1)} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix}$$

#### 4.5 ì²« ë²ˆì§¸ ë ˆì´ì–´ ê³„ì‚°
$$H^{(1)} = \text{ReLU}(\hat{A}XW^{(0)}) = \text{ReLU}\left(\begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & 0 \\ \frac{1}{\sqrt{6}} & \frac{1}{3} & \frac{1}{\sqrt{6}} \\ 0 & \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 3 & 1 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \end{bmatrix}\right)$$

$$= \text{ReLU}\left(\begin{bmatrix} 2.45 & 1.63 & 4.08 \\ 1.63 & 1.09 & 2.72 \\ 1.22 & 2.45 & 3.67 \end{bmatrix}\right) = \begin{bmatrix} 2.45 & 1.63 & 4.08 \\ 1.63 & 1.09 & 2.72 \\ 1.22 & 2.45 & 3.67 \end{bmatrix}$$

#### 4.6 ë‘ ë²ˆì§¸ ë ˆì´ì–´ ê³„ì‚°
$$Z = \text{softmax}(\hat{A}H^{(1)}W^{(1)}) = \text{softmax}\left(\begin{bmatrix} 3.67 & 5.72 \\ 2.45 & 4.08 \\ 3.67 & 6.12 \end{bmatrix}\right)$$

---

## 5. ë…¼ë¦¬ì  ì¦ëª… ë˜ëŠ” ëª¨ë¸ íŠ¹ì„± ë¶„ì„ (Proofs or Analysis)

### 5.1 ê³„ì‚° ë³µì¡ë„ ë¶„ì„

**ì‹œê°„ ë³µì¡ë„**: $O(|E|d^2)$
- í–‰ë ¬ ê³±ì…ˆ: $\hat{A}H^{(l)}W^{(l)}$
- $\hat{A}$ëŠ” sparse matrix (sparse-dense ê³±ì…ˆ)
- $|E|$: ì—£ì§€ ìˆ˜, $d$: feature ì°¨ì›

**ê³µê°„ ë³µì¡ë„**: $O(Nd + |E|)$
- ë…¸ë“œ feature: $O(Nd)$
- ì¸ì ‘ í–‰ë ¬: $O(|E|)$

### 5.2 Spectral íŠ¹ì„± ë¶„ì„

**ëª…ì œ**: GCNì€ low-pass filter ì—­í• ì„ í•œë‹¤

**ì¦ëª…**:
1. $\hat{A} = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ì˜ ê³ ìœ ê°’ì€ $[-1, 1]$ ë²”ìœ„
2. ê³ ì£¼íŒŒ ì‹ í˜¸(ê³ ìœ ê°’ì´ í°)ëŠ” ê°ì‡ 
3. ì €ì£¼íŒŒ ì‹ í˜¸(ê³ ìœ ê°’ì´ ì‘ì€)ëŠ” ë³´ì¡´

### 5.3 Weisfeiler-Lehman ì•Œê³ ë¦¬ì¦˜ê³¼ì˜ ì—°ê²°

**ëª…ì œ**: GCNì€ WL ì•Œê³ ë¦¬ì¦˜ì˜ neural generalization

**ì¦ëª…**:
1. WL ì•Œê³ ë¦¬ì¦˜: $h_i^{(l+1)} = \text{hash}(h_i^{(l)}, \{h_j^{(l)} : j \in \mathcal{N}_i\})$
2. GCN: $h_i^{(l+1)} = \sigma\left(\sum_{j \in \mathcal{N}_i} \frac{1}{\sqrt{|\mathcal{N}_i||\mathcal{N}_j|}} W^{(l)}h_j^{(l)}\right)$
3. ë‘ ë°©ë²• ëª¨ë‘ ì´ì›ƒ ì •ë³´ë¥¼ ì§‘ê³„í•˜ì—¬ ë…¸ë“œ í‘œí˜„ ì—…ë°ì´íŠ¸

### 5.4 ìˆ˜ì¹˜ì  ì•ˆì •ì„± ì¦ëª…

**ëª…ì œ**: Renormalization trickì´ ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ë³´ì¥

**ì¦ëª…**:
1. $\tilde{A} = A + I_N$ìœ¼ë¡œ self-loop ì¶”ê°€
2. $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ì˜ ê³ ìœ ê°’ì´ $[0, 2]$ ë²”ìœ„
3. ê³ ìœ ê°’ì´ 0ì´ ë˜ì§€ ì•Šì•„ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´

---

## 6. ì‹¤í—˜ ì„¤ì •ê³¼ ê²°ê³¼ ìš”ì•½ (Experiments & Results)

### 6.1 ë°ì´í„°ì…‹ íŠ¹ì„±

| ë°ì´í„°ì…‹ | ë…¸ë“œ ìˆ˜ | ì—£ì§€ ìˆ˜ | í´ë˜ìŠ¤ ìˆ˜ | Feature ìˆ˜ | ë¼ë²¨ ë¹„ìœ¨ |
|---------|---------|---------|-----------|------------|-----------|
| Cora | 2,708 | 5,429 | 7 | 1,433 | 5.2% |
| Citeseer | 3,327 | 4,732 | 6 | 3,703 | 3.6% |
| Pubmed | 19,717 | 44,338 | 3 | 500 | 0.3% |
| NELL | 65,755 | 266,144 | 210 | 5,414 | 0.1% |

### 6.2 ëª¨ë¸ ì„¤ì •

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- Learning rate: 0.01
- Dropout: 0.5
- L2 regularization: 5e-4
- Hidden units: 16 (NELL: 64)
- Optimizer: Adam
- Early stopping: 200 epochs

**ëª¨ë¸ êµ¬ì¡°**:
- 2-layer GCN
- Layer 1: ReLU activation
- Layer 2: Softmax activation

### 6.3 Baseline ë¹„êµ

**ë¹„êµ ëŒ€ìƒ**:
- Label Propagation
- SemiEmb
- ManiReg
- DeepWalk
- ICA
- Planetoid
- Chebyshev
- MLP

### 6.4 ì„±ëŠ¥ ê²°ê³¼

| ë°©ë²• | Cora | Citeseer | Pubmed | NELL |
|------|------|----------|--------|------|
| GCN | 81.5% | 70.3% | 79.0% | 66.0% |
| Planetoid* | 75.7% | 64.7% | 77.2% | 61.9% |
| DeepWalk | 70.7% | 51.4% | 74.3% | 58.1% |
| ICA | 75.1% | 69.1% | 73.9% | 66.0% |

**ì£¼ìš” ì„±ê³¼**:
- ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ ë‹¬ì„±
- ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ (wall-clock time ë‹¨ì¶•)
- Semi-supervised settingì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥

### 6.5 ì¶”ê°€ ì‹¤í—˜ ê²°ê³¼

**Karate Club ì˜ˆì œ**:
- 34ê°œ ë…¸ë“œ, 2ê°œ í´ë˜ìŠ¤
- 4ê°œ ë…¸ë“œë§Œ ë¼ë²¨ ì‚¬ìš©
- GCNì´ í´ëŸ¬ìŠ¤í„° êµ¬ì¡°ë¥¼ ì˜ í¬ì°©

**Random Weight GCN**:
- í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì—†ì´ë„ ê°•ë ¥í•œ feature extractor
- ê·¸ë˜í”„ êµ¬ì¡° ì •ë³´ë§Œìœ¼ë¡œë„ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ í•™ìŠµ

---

## 7. í•œê³„ì ê³¼ í–¥í›„ ë°©í–¥ (Limitations & Future Directions)

### 7.1 í˜„ì¬ í•œê³„ì 

**ê³„ì‚°ì  í•œê³„**:
- Full-batch í•™ìŠµìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
- ëŒ€ê·œëª¨ ê·¸ë˜í”„ì—ì„œ mini-batch SGD í•„ìš”
- $O(|E|d^2)$ ê³„ì‚° ë³µì¡ë„

**êµ¬ì¡°ì  í•œê³„**:
- 1-hop ì´ì›ƒë§Œ ê³ ë ¤ (ê³ ì°¨ ì´ì›ƒ ì •ë³´ ì†ì‹¤)
- Undirected graphë§Œ ì§€ì›
- Edge feature ë¯¸ì‚¬ìš©
- Self-loopì™€ ì´ì›ƒ ê°„ ì¤‘ìš”ë„ ê· í˜• ë¬¸ì œ

**ì‹¤ìš©ì  í•œê³„**:
- Transductive settingì—ì„œë§Œ ê²€ì¦
- Inductive settingì—ì„œ ì„±ëŠ¥ ì €í•˜
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ over-smoothing ë¬¸ì œ

### 7.2 í–¥í›„ ì—°êµ¬ ë°©í–¥

**í™•ì¥ì„± ê°œì„ **:
- Mini-batch í•™ìŠµ ë°©ë²• ê°œë°œ
- Sparse matrix ì—°ì‚° ìµœì í™”
- Graph partitioning ê¸°ë°˜ ë³‘ë ¬í™”

**ê¸°ëŠ¥ í™•ì¥**:
- Directed graph ì§€ì›: $A_{ij} \neq A_{ji}$
- Edge feature í†µí•©: $H^{(l+1)} = \sigma(\hat{A}H^{(l)}W^{(l)} + E^{(l)}W_E^{(l)})$
- ê³ ì°¨ ì´ì›ƒ attention: $H^{(l+1)} = \sigma(\sum_{k=1}^K \alpha_k \hat{A}^k H^{(l)}W^{(l)})$

**ëª¨ë¸ ê°œì„ **:
- Residual connection: $H^{(l+1)} = \sigma(\hat{A}H^{(l)}W^{(l)}) + H^{(l)}$
- Batch normalization: $H^{(l+1)} = \text{BN}(\sigma(\hat{A}H^{(l)}W^{(l)}))$
- Attention mechanism ë„ì…

**í•´ì„ ê°€ëŠ¥ì„±**:
- Graph visualization ë„êµ¬
- ì¤‘ìš” ë…¸ë“œ/ì—£ì§€ ì‹ë³„
- Feature importance ë¶„ì„

---

## 8. í•œëˆˆì— ë³´ëŠ” ìš”ì•½ (TL;DR)

**GCNì€ Chebyshev ë‹¤í•­ì‹ ê·¼ì‚¬ì™€ renormalization trickì„ í†µí•´ spectral graph convolutionì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•œ semi-supervised ë…¸ë“œ ë¶„ë¥˜ ëª¨ë¸ì´ë‹¤. í•µì‹¬ ìˆ˜ì‹ $H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$ë¡œ 1-hop ì´ì›ƒ ì •ë³´ë¥¼ ì§‘ê³„í•˜ë©°, $O(|E|d^2)$ ê³„ì‚° ë³µì¡ë„ë¡œ ê¸°ì¡´ spectral ë°©ë²•ì˜ $O(N^3)$ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. Cora, Citeseer, Pubmed ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, Weisfeiler-Lehman ì•Œê³ ë¦¬ì¦˜ì˜ neural generalizationìœ¼ë¡œ í•´ì„ëœë‹¤. ë‹¤ë§Œ transductive setting ì œí•œ, edge feature ë¯¸ì‚¬ìš©, over-smoothing ë¬¸ì œ ë“± ê°œì„  ì—¬ì§€ê°€ ìˆë‹¤.**