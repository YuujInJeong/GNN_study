# ğŸ“˜ Graph Attention Networks (GAT) 

## 1. ì—°êµ¬ ë¬¸ì œ ì •ì˜ì™€ ë™ê¸° (Problem & Motivation)

### í•µì‹¬ ë¬¸ì œ
> **ê·¸ë˜í”„ êµ¬ì¡° ë°ì´í„°ì—ì„œ ë…¸ë“œ ê°„ ê´€ê³„ì˜ ì¤‘ìš”ë„ë¥¼ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë¬¸ì œ**
- ê¸°ì¡´ GCNì˜ í•œê³„: ëª¨ë“  ì´ì›ƒ ë…¸ë“œì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ ì ìš©
- ê³ ìœ ë¶„í•´(eigendecomposition) ê°™ì€ ê³ ë¹„ìš© ì—°ì‚° í•„ìš”ì„±

### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì 
1. **GCN**: $\tilde{h}_i^{(l+1)} = \sigma\left(\sum_{j \in \mathcal{N}_i} \frac{1}{\sqrt{|\mathcal{N}_i||\mathcal{N}_j|}} W^{(l)} \tilde{h}_j^{(l)}\right)$
   - ëª¨ë“  ì´ì›ƒì— ë™ì¼í•œ ì •ê·œí™” ê°€ì¤‘ì¹˜ ì ìš©
   - ê·¸ë˜í”„ êµ¬ì¡°ì— ì˜ì¡´ì  (transductive)

2. **GraphSAGE**: ë‹¤ì–‘í•œ ì§‘ê³„ í•¨ìˆ˜ ì‚¬ìš©í•˜ì§€ë§Œ ì´ì›ƒë³„ ì¤‘ìš”ë„ êµ¬ë¶„ ì—†ìŒ

### GATì˜ í•´ê²° ë°©í–¥
- **Self-attention ë©”ì»¤ë‹ˆì¦˜**ìœ¼ë¡œ ì´ì›ƒë³„ ì¤‘ìš”ë„ ìë™ í•™ìŠµ
- **Inductive setting**ì—ì„œ unseen ê·¸ë˜í”„ì—ë„ ì ìš© ê°€ëŠ¥
- **ë³‘ë ¬ ê³„ì‚°**ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ

---

## 2. ì´ë¡ ì  ë°°ê²½ ë° í•µì‹¬ ê°œë… (Background & Definitions)

### ê·¸ë˜í”„ ì •ì˜
- **ê·¸ë˜í”„**: $G = (V, E)$
  - $V$: ë…¸ë“œ ì§‘í•©, $|V| = N$
  - $E$: ì—£ì§€ ì§‘í•©
- **ë…¸ë“œ feature**: $\mathbf{h} = \{\tilde{h}_1, \tilde{h}_2, ..., \tilde{h}_N\}$, $\tilde{h}_i \in \mathbb{R}^F$
- **ì´ì›ƒ ì§‘í•©**: $\mathcal{N}_i = \{j \in V : (i,j) \in E\}$

### Attention ë©”ì»¤ë‹ˆì¦˜
- **Attention score**: ë…¸ë“œ ìŒ $(i,j)$ ê°„ì˜ ì¤‘ìš”ë„ ì¸¡ì •
- **Attention weight**: softmaxë¡œ ì •ê·œí™”ëœ ìµœì¢… ê°€ì¤‘ì¹˜
- **Multi-head attention**: ì—¬ëŸ¬ ë…ë¦½ì ì¸ attention ë©”ì»¤ë‹ˆì¦˜ ë³‘í•©

### í•µì‹¬ ê°€ì •
- **1-hop ì´ì›ƒë§Œ ê³ ë ¤**: ì§ì ‘ ì—°ê²°ëœ ë…¸ë“œë§Œ attention ê³„ì‚°
- **ë§ˆìŠ¤í‚¹**: adjacency matrixë¡œ ì—°ê²°ë˜ì§€ ì•Šì€ ë…¸ë“œ ìŒ ì œì™¸

---

## 3. ëª¨ë¸ êµ¬ì¡° ë° í•µì‹¬ ìˆ˜ì‹ íë¦„ (Model Architecture & Equations)

### ì „ì²´ ëª¨ë¸ êµ¬ì¡°
```
ì…ë ¥ ë…¸ë“œ feature â†’ ì„ í˜• ë³€í™˜ â†’ Attention score ê³„ì‚° â†’ ë§ˆìŠ¤í‚¹ â†’ Softmax â†’ ê°€ì¤‘í•© â†’ ì¶œë ¥
```

### ë‹¨ì¼ Attention Head ìˆ˜ì‹ íë¦„

#### 3.1 ì„ í˜• ë³€í™˜ (Linear Transformation)
$$\mathbf{h}'_i = W\mathbf{h}_i, \quad W \in \mathbb{R}^{F' \times F}$$

**ì—­í• **: ë…¸ë“œ featureë¥¼ ë” ë†’ì€ ì¶”ìƒ í‘œí˜„ìœ¼ë¡œ íˆ¬ì˜

#### 3.2 Attention Score ê³„ì‚°
$$e_{ij} = \text{LeakyReLU}\left(\mathbf{a}^T [W\mathbf{h}_i \| W\mathbf{h}_j]\right)$$

**êµ¬ì„± ìš”ì†Œ**:
- $\mathbf{a} \in \mathbb{R}^{2F'}$: í•™ìŠµ ê°€ëŠ¥í•œ attention vector
- $\|$: feature concatenation
- $\text{LeakyReLU}(x) = \max(0.2x, x)$: ìŒìˆ˜ ê¸°ìš¸ê¸° ì •ë³´ ë³´ì¡´

#### 3.3 ë§ˆìŠ¤í‚¹ (Masking)
$$e_{ij} = -\infty \quad \text{if } j \notin \mathcal{N}_i$$

**ëª©ì **: ì—°ê²°ë˜ì§€ ì•Šì€ ë…¸ë“œ ìŒì˜ attention scoreë¥¼ 0ìœ¼ë¡œ ë§Œë“¦

#### 3.4 Softmax ì •ê·œí™”
$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}$$

**íŠ¹ì„±**: $\sum_{j \in \mathcal{N}_i} \alpha_{ij} = 1$ (í™•ë¥  ë¶„í¬)

#### 3.5 ìµœì¢… ì¶œë ¥ ê³„ì‚°
$$\mathbf{h}'_i = \sigma\left(\sum_{j \in \mathcal{N}_i} \alpha_{ij} W\mathbf{h}_j\right)$$

**í•´ì„**: ì¤‘ìš”í•œ ì´ì›ƒì¼ìˆ˜ë¡ ë” í° ê°€ì¤‘ì¹˜ë¡œ feature ì§‘ê³„

### Multi-head Attention í™•ì¥

#### 3.6 Kê°œ Head ë³‘í•© (Concatenation)
$$\mathbf{h}'_i = \|_{k=1}^K \sigma\left(\sum_{j \in \mathcal{N}_i} \alpha_{ij}^k W^k\mathbf{h}_j\right)$$

**ì¶œë ¥ ì°¨ì›**: $KF'$

#### 3.7 Kê°œ Head í‰ê·  (Averaging)
$$\mathbf{h}'_i = \sigma\left(\frac{1}{K}\sum_{k=1}^K \sum_{j \in \mathcal{N}_i} \alpha_{ij}^k W^k\mathbf{h}_j\right)$$

**ì¶œë ¥ ì°¨ì›**: $F'$

---

## 4. í•µì‹¬ ìˆ˜ì‹ì˜ ì˜ˆì‹œì™€ í•´ì„ (Worked Example)

### ì˜ˆì‹œ ê·¸ë˜í”„ ì„¤ì •
```
ë…¸ë“œ: A, B, C (3ê°œ)
ì—£ì§€: A-B, A-C
Feature ì°¨ì›: F=2, F'=3
```

### ì…ë ¥ ë°ì´í„°
$$\mathbf{h}_A = [1, 2], \quad \mathbf{h}_B = [3, 1], \quad \mathbf{h}_C = [0, 4]$$

### ê°€ì¤‘ì¹˜ í–‰ë ¬ (ì˜ˆì‹œ)
$$W = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix}, \quad \mathbf{a} = [0.5, 0.3, -0.2]$$

### ë‹¨ê³„ë³„ ê³„ì‚°

#### 4.1 ì„ í˜• ë³€í™˜
$$W\mathbf{h}_A = [1, 2, 3], \quad W\mathbf{h}_B = [3, 1, 4], \quad W\mathbf{h}_C = [0, 4, 4]$$

#### 4.2 Attention Score ê³„ì‚° (ë…¸ë“œ A ê¸°ì¤€)
$$e_{AB} = \text{LeakyReLU}([0.5, 0.3, -0.2]^T [1,2,3,3,1,4]) = \text{LeakyReLU}(0.5 \cdot 1 + 0.3 \cdot 2 + (-0.2) \cdot 3 + 0.5 \cdot 3 + 0.3 \cdot 1 + (-0.2) \cdot 4) = \text{LeakyReLU}(1.4) = 1.4$$

$$e_{AC} = \text{LeakyReLU}([0.5, 0.3, -0.2]^T [1,2,3,0,4,4]) = \text{LeakyReLU}(0.5 \cdot 1 + 0.3 \cdot 2 + (-0.2) \cdot 3 + 0.5 \cdot 0 + 0.3 \cdot 4 + (-0.2) \cdot 4) = \text{LeakyReLU}(0.9) = 0.9$$

#### 4.3 Softmax ì •ê·œí™”
$$\alpha_{AB} = \frac{\exp(1.4)}{\exp(1.4) + \exp(0.9)} = \frac{4.06}{4.06 + 2.46} = 0.62$$

$$\alpha_{AC} = \frac{\exp(0.9)}{\exp(1.4) + \exp(0.9)} = \frac{2.46}{4.06 + 2.46} = 0.38$$

#### 4.4 ìµœì¢… ì¶œë ¥
$$\mathbf{h}'_A = \sigma(0.62 \cdot [3,1,4] + 0.38 \cdot [0,4,4]) = \sigma([1.86, 2.54, 3.84])$$

---

## 5. ë…¼ë¦¬ì  ì¦ëª… ë˜ëŠ” ëª¨ë¸ íŠ¹ì„± ë¶„ì„ (Proofs or Analysis)

### 5.1 Permutation Invariance ì¦ëª…

**ëª…ì œ**: GATëŠ” ë…¸ë“œ ìˆœì„œì— ë¬´ê´€í•˜ë‹¤ (permutation invariant)

**ì¦ëª…**:
1. Attention score: $e_{ij} = f(W\mathbf{h}_i, W\mathbf{h}_j)$
2. ë…¸ë“œ ìˆœì„œ ë³€ê²½ ì‹œ $f(W\mathbf{h}_i, W\mathbf{h}_j) = f(W\mathbf{h}_j, W\mathbf{h}_i)$ (ëŒ€ì¹­ì„±)
3. SoftmaxëŠ” ìˆœì„œì— ë¬´ê´€í•˜ë¯€ë¡œ $\alpha_{ij}$ë„ ìˆœì„œì— ë¬´ê´€
4. ìµœì¢… ì¶œë ¥ë„ ìˆœì„œì— ë¬´ê´€

### 5.2 ê³„ì‚° ë³µì¡ë„ ë¶„ì„

**ì‹œê°„ ë³µì¡ë„**: $O(|V|FF' + |E|F')$
- ì„ í˜• ë³€í™˜: $O(|V|FF')$
- Attention ê³„ì‚°: $O(|E|F')$ (sparse graphì—ì„œ $|E| \ll |V|^2$)

**ê³µê°„ ë³µì¡ë„**: $O(|V|F' + |E|)$
- ë…¸ë“œ feature: $O(|V|F')$
- Attention weight: $O(|E|)$

### 5.3 Inductive íŠ¹ì„± ì¦ëª…

**ëª…ì œ**: GATëŠ” unseen ë…¸ë“œì—ë„ ì ìš© ê°€ëŠ¥

**ì¦ëª…**:
1. Attention scoreëŠ” ë…¸ë“œ featureë§Œìœ¼ë¡œ ê³„ì‚°
2. ê·¸ë˜í”„ êµ¬ì¡° ì •ë³´ëŠ” ë§ˆìŠ¤í‚¹ì—ë§Œ ì‚¬ìš©
3. ìƒˆë¡œìš´ ë…¸ë“œê°€ ì¶”ê°€ë˜ì–´ë„ ê¸°ì¡´ ê°€ì¤‘ì¹˜ $W, \mathbf{a}$ ì¬ì‚¬ìš© ê°€ëŠ¥

---

## 6. ì‹¤í—˜ ì„¤ì •ê³¼ ê²°ê³¼ ìš”ì•½ (Experiments & Results)

### 6.1 ë°ì´í„°ì…‹ íŠ¹ì„±

| ë°ì´í„°ì…‹ | ë…¸ë“œ ìˆ˜ | ì—£ì§€ ìˆ˜ | í´ë˜ìŠ¤ ìˆ˜ | Feature ìˆ˜ | í•™ìŠµ ë°©ì‹ |
|---------|---------|---------|-----------|------------|-----------|
| Cora | 2,708 | 5,429 | 7 | 1,433 | Transductive |
| Citeseer | 3,327 | 4,732 | 6 | 3,703 | Transductive |
| Pubmed | 19,717 | 44,338 | 3 | 500 | Transductive |
| PPI | ~2,372 | - | 121 (ë‹¤ì¤‘) | 50 | Inductive |

### 6.2 ëª¨ë¸ ì„¤ì •

**Transductive ì„¤ì •**:
- 2-layer GAT
- Layer 1: 8-head attention, ELU activation
- Layer 2: 1-head attention, softmax
- Dropout: 0.6, L2 regularization: 0.0005

**Inductive ì„¤ì •**:
- 3-layer GAT
- Layer 1-2: 4-head attention
- Layer 3: 6-head attention, logistic sigmoid
- Skip connection, Adam optimizer

### 6.3 ì„±ëŠ¥ ê²°ê³¼

| ë°ì´í„°ì…‹ | GCN | GAT | í–¥ìƒë„ |
|---------|-----|-----|--------|
| Cora | 81.4% | 83.0% | +1.6% |
| Citeseer | 70.3% | 72.5% | +2.2% |
| Pubmed | 79.0% | 79.0% | 0.0% |
| PPI (micro-F1) | 50.0% | 97.3% | +47.3% |

### 6.4 Attention Weight ë¶„ì„
- t-SNE ì‹œê°í™”ì—ì„œ í´ëŸ¬ìŠ¤í„° í˜•ì„± í™•ì¸
- ì¤‘ìš”í•œ ì´ì›ƒì— ë†’ì€ attention weight í• ë‹¹
- Multi-head attentionì´ ë‹¤ì–‘í•œ ê´€ê³„ íŒ¨í„´ í¬ì°©

---

## 7. í•œê³„ì ê³¼ í–¥í›„ ë°©í–¥ (Limitations & Future Directions)

### 7.1 í˜„ì¬ í•œê³„ì 

**ê³„ì‚°ì  í•œê³„**:
- Attention ê³„ì‚°ì´ $O(|E|F')$ë¡œ ì—£ì§€ ìˆ˜ì— ë¹„ë¡€
- ëŒ€ê·œëª¨ ê·¸ë˜í”„ì—ì„œ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±

**êµ¬ì¡°ì  í•œê³„**:
- 1-hop ì´ì›ƒë§Œ ê³ ë ¤ (ê³ ì°¨ ì´ì›ƒ ì •ë³´ ì†ì‹¤)
- Edge feature ë¯¸ì‚¬ìš©
- Attention weight í•´ì„ ì–´ë ¤ì›€

**ì‹¤ìš©ì  í•œê³„**:
- Hyperparameter tuning ë³µì¡ì„±
- Attention weightì˜ ë¶ˆì•ˆì •ì„±

### 7.2 í–¥í›„ ì—°êµ¬ ë°©í–¥

**í™•ì¥ì„± ê°œì„ **:
- Sparse attention êµ¬í˜„
- Hierarchical attention êµ¬ì¡°
- Graph partitioning ê¸°ë°˜ ë³‘ë ¬í™”

**ê¸°ëŠ¥ í™•ì¥**:
- Edge feature í†µí•©: $e_{ij} = f(W\mathbf{h}_i, W\mathbf{h}_j, W_e\mathbf{e}_{ij})$
- ê³ ì°¨ ì´ì›ƒ attention: $e_{ij}^{(l)} = f(\mathbf{h}_i^{(l-1)}, \mathbf{h}_j^{(l-1)})$
- Graph-level attention

**í•´ì„ ê°€ëŠ¥ì„±**:
- Attention weight ì‹œê°í™” ë„êµ¬
- ì¤‘ìš” ë…¸ë“œ/ì—£ì§€ ì‹ë³„ ì•Œê³ ë¦¬ì¦˜
- Attention pattern ë¶„ì„

---

## 8. í•œëˆˆì— ë³´ëŠ” ìš”ì•½ (TL;DR)

**GATëŠ” self-attention ë©”ì»¤ë‹ˆì¦˜ì„ ê·¸ë˜í”„ì— ì ìš©í•˜ì—¬ ë…¸ë“œ ê°„ ê´€ê³„ì˜ ì¤‘ìš”ë„ë¥¼ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” GNN ëª¨ë¸ì´ë‹¤. í•µì‹¬ ìˆ˜ì‹ $e_{ij} = \text{LeakyReLU}(\mathbf{a}^T[W\mathbf{h}_i \| W\mathbf{h}_j])$ë¡œ attention scoreë¥¼ ê³„ì‚°í•˜ê³ , $\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}$ë¡œ ì •ê·œí™”í•˜ì—¬ ìµœì¢… ì¶œë ¥ $\mathbf{h}'_i = \sigma(\sum_{j \in \mathcal{N}_i} \alpha_{ij} W\mathbf{h}_j)$ë¥¼ ìƒì„±í•œë‹¤. GCN ëŒ€ë¹„ 1.5-2.2% ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ë©°, inductive settingì—ì„œë„ ìš°ìˆ˜í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì¸ë‹¤. ë‹¤ë§Œ ê³„ì‚° ë³µì¡ë„ì™€ attention weight í•´ì„ì„± ì¸¡ë©´ì—ì„œ ê°œì„  ì—¬ì§€ê°€ ìˆë‹¤.**