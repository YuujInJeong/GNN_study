
## Chapter 10 그래프 신경망을 이용한 링크 예측

---

### 1. 링크 예측(Link Prediction) 개요

---

- **정의**: 두 노드 사이의 링크 존재 여부를 예측하는 문제.
- **활용 예시**:
    - 소셜 네트워크에서 새로운 친구 추천
    - 추천 시스템에서 관심사 기반 아이템 추천
- **의미**: 높은 연결 가능성을 가진 노드를 파악하여 네트워크 구조 이해 및 추천 시스템 강화.

### 2. 학습 접근 방식

---

링크 예측 방법은 크게 세 가지로 나뉨

1. **전통적 방법**
    - 휴리스틱 기법
    - 행렬 분해(Matrix Factorization)
2. **노드 임베딩 기반 방법**
    - DeepWalk, Node2Vec 등 사용
3. **GNN 기반 방법**
    - 노드 및 서브그래프 단위 예측
    - PyTorch Geometric 등으로 구현

## 3. 전통적 방법

### (1) 휴리스틱 기법

- 목표 노드와 인접한 이웃(1-hop, 2-hop 등)을 기반으로 유사성을 측정.

- **주요 지표**
    
    
    **공통 이웃(Common Neighbors)**
    
    $$
    f(u,v)=∣N(u)∩N(v)∣
    $$
    
    → 두 노드 사이에 공유된 이웃의 수.
    

**자카드 계수(Jaccard Coefficient)**

$$
f(u,v) = \frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}
$$

→ 공통 이웃 비율을 측정.

**아다믹-아다르 지수(Adamic-Adar Index)**

$$
f(u,v) = \sum_{x \in N(u) \cap N(v)} \frac{1}{\log |N(x)|}
$$

→ 드문(high-value) 이웃에 더 큰 가중치 부여.
→ 공통된 항목이 얼마나 희귀한지를 반영하여 가중치를 부여함.

### (2) 전역 휴리스틱

- **카츠 인덱스(Katz Index)**
    
    $$
    f(u,v) = \sum_{i=1}^{\infty} \beta^i A^i
    $$
    
    → 모든 경로를 고려하되 길이에 따라 감쇠
    
    - AA: 그래프의 인접 행렬
    - (Ai)uv(A^i)_{uv}: 노드 uu에서 vv까지 **길이가 i인 경로의 개수**
    - β\beta: 감쇠 계수(discount factor), 0<β<1/λmax⁡(A)0 < \beta < 1/\lambda_{\max}(A)
        - 여기서 λmax⁡(A)\lambda_{\max}(A)는 인접행렬의 최대 고유값
        - 이 조건을 만족해야 수열이 수렴함
    
    ### 의미
    
    - **경로의 수**: 모든 가능한 경로를 고려
    - **경로 길이에 따른 감쇠**: 경로 길이가 길수록 βi\beta^i 때문에 영향력이 작아짐
    - **직관**:
        - 가까운 경로(짧은 길이)가 더 강한 영향을 줌
        - 하지만 긴 경로도 일정 비율로 반영하여 구조적 연관성을 고려
    
    ### 특징
    
    - 모든 경로를 고려 → **전역적(global)** 특성 반영
    - β\beta가 크면 긴 경로 영향이 커짐, 작으면 짧은 경로 위주로 반영

- **재시작 랜덤 워크(Random Walk with Restart)**
    
    → 노드에서 시작해 무작위 이동하며 방문 확률을 누적.
    

---

r=(1−α)(I−αP)−1eur = (1-\alpha)(I - \alpha P)^{-1} e_u

여기서:

- PP: 확률 전이 행렬 (행렬의 각 원소는 이동 확률)
- α\alpha: 재시작 확률 (일반적으로 0.7~0.9)
- eue_u: 시작 노드 uu를 나타내는 원-핫 벡터
- rr: 각 노드가 방문될 확률 분포 (steady-state distribution)

### 해석

- 매 스텝에서:
    - 확률 α\alpha로 인접 노드로 이동
    - 확률 1−α1-\alpha로 다시 시작 노드 uu로 돌아감
- 무한히 반복하면 수렴한 확률 분포 rr을 얻음

### 의미

- **지역적(local) 중요성 반영**: 시작 노드 근처에서 자주 방문되는 노드일수록 확률이 큼
- **재시작 확률 α\alpha**:
    - 크면 시작 노드에 머무는 경향 ↑
    - 작으면 멀리 퍼져나감

### 특징

- Personalized PageRank와 유사한 개념
- 링크 예측에서는 “시작 노드 uu”와 가장 많이 함께 등장할 가능성이 큰 vv를 찾는 방식

## 1. 점화식 형태 (틸다 기호와 함께 쓰이는 경우)

RWR의 확률 분포 r(t)r^{(t)}를 반복적으로 업데이트하는 식으로 표현하면:

r(t+1)=αPTr(t)+(1−α)eur^{(t+1)} = \alpha P^T r^{(t)} + (1-\alpha)e_u

여기서:

- r(t)r^{(t)}: tt번째 스텝에서의 확률 분포
- PTP^T: 전이 확률 행렬의 전치 (확률이 열 기준일 때)
- α\alpha: 이동 확률
- 1−α1-\alpha: 재시작 확률
- eue_u: 시작 노드를 나타내는 원-핫 벡터

→ 충분히 큰 tt에 대해:

r(t)      ~    rr^{(t)} \;\;\tilde{\;}\;\; r

즉, “틸다(~)”를 써서 **수렴 근사 (approximation to steady state)**를 표현하는 거예요.

---

## 2. 해석

- 위 식은 **반복 알고리즘(iterative update)**:
    
    시작 분포에서 조금씩 업데이트 하다가 최종적으로 수렴 상태(steady state)에 가까워짐.
    
- “~” 기호는 t→∞t \to \infty 일 때
    
    r(t)∼rr^{(t)} \sim r
    
    로 표기하여, r(t)r^{(t)}가 최종 분포 rr에 가까워진다는 뜻.
    

---

## 3. 행렬 해석과 연결

점화식을 무한 반복하면, 제가 앞서 말한 닫힌형(closed form):

r=(1−α)(I−αPT)−1eur = (1-\alpha)(I - \alpha P^T)^{-1} e_u

와 동일한 결과에 도달합니다.

즉:

- **실제 구현**에서는 보통 반복적 업데이트(틸다 버전)를 사용 (계산 효율 ↑)
- **이론적 설명**에서는 닫힌형 수식을 제시

---

---

## 4. 행렬 분해(Matrix Factorization)

- **아이디어**: 인접 행렬 A를 노드 임베딩 벡터 Z의 곱으로 근사.
    
    $$
    A \approx Z^T Z
    $$
    
- **설명**:
    - 노드 간 유사성이 높으면 내적 zuTzvz_u^T z_v가 커져야 함.
    - 추천 시스템 및 링크 예측에서 활용.

---

## 5. 노드 임베딩을 활용한 링크 예측

- GNN을 통해 노드 임베딩을 생성한 후, 이를 기반으로 링크 존재 확률을 예측.

## 5.1 기본 개념

노드 임베딩을 활용한 링크 예측은 그래프의 인접 행렬을 노드 임베딩의 내적으로 근사하여, 두 노드가 연결될 가능성을 추정하는 방법이다.

- **인접 행렬 AA**
    - Auv=1A_{uv} = 1: 노드 u,vu, v가 연결됨
    - Auv=0A_{uv} = 0: 연결되지 않음
- **임베딩 행렬 ZZ**
    - 각 노드 uu를 벡터 zu∈Rdz_u \in \mathbb{R}^d로 표현
    - 유사한 노드는 임베딩 공간에서도 가깝게 위치

**핵심 가정**:

노드 간 임베딩 내적 zuTzvz_u^T z_v 값이 클수록 실제로 연결될 확률이 높다.

---

## 5.2 행렬 분해(Matrix Factorization)

### (1) 목적 함수

min⁡Z∑i∈V,j∈V(Aij−ziTzj)2\min_{Z} \sum_{i \in V, j \in V} (A_{ij} - z_i^T z_j)^2

- 실제 연결 여부 AijA_{ij}와 예측값 ziTzjz_i^T z_j 차이를 최소화
- 추천 시스템의 협업 필터링(Matrix Factorization)과 동일한 아이디어

### (2) 해석

- AijA_{ij}가 1인 경우 → ziTzjz_i^T z_j가 크도록 학습
- AijA_{ij}가 0인 경우 → ziTzjz_i^T z_j가 작도록 학습

---

## 5.3 랜덤 워크 기반 점수와의 연결

DeepWalk / Node2Vec 같은 방법에서는 단순히 AA를 쓰지 않고,

랜덤 워크 기반으로 얻은 **공존 확률 행렬 MM**을 만든다:

Mij=log⁡(∑r=1T(D−1A)r)−log⁡bM_{ij} = \log \left( 
\sum_{r=1}^{T} (D^{-1}A)^r 
\right) - \log b

- (D−1A)r(D^{-1}A)^r: r-step 랜덤 워크 확률
- TT: 최대 걸음 수
- −log⁡b\log b: 정규화 항
- 직관: **멀리 떨어져 있어도 랜덤 워크에서 자주 만나는 노드 쌍은 높은 점수**

이 MM을 타겟으로 하여 행렬 분해를 수행한다:

min⁡Z∑i,j(Mij−ziTzj)2\min_{Z} \sum_{i,j} (M_{ij} - z_i^T z_j)^2

---

## 5.4 예시: 7노드 그래프

### (1) 그래프 구조

```
1 -- 2 -- 4 -- 6
 \   |     |
  \  3 -- 5 -- 7

```

### (2) 인접 행렬 A

$$
A =
\begin{bmatrix}
0 & 1 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 & 1 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 & 1 & 0 \\
\end{bmatrix}
$$

### (3) 차수 행렬 D

$$
D =
\begin{bmatrix}
2 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 3 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 3 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 2 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 2 \\
\end{bmatrix}
$$

---

### (4) 1-step 랜덤 워크 확률 행렬

$$
P=D^{-1}A
$$

$$
P =
\begin{bmatrix}
0 & 0.5 & 0.5 & 0   & 0   & 0   & 0   \\
0.33 & 0   & 0.33 & 0.33 & 0   & 0   & 0   \\
0.33 & 0.33 & 0   & 0   & 0.33 & 0   & 0   \\
0   & 0.33 & 0   & 0   & 0.33 & 0.33 & 0   \\
0   & 0   & 0.33 & 0.33 & 0   & 0   & 0.33 \\
0   & 0   & 0   & 0.5 & 0   & 0   & 0.5 \\
0   & 0   & 0   & 0   & 0.5 & 0.5 & 0   \\
\end{bmatrix}
$$

### (5) 2-step 랜덤 워크 확률 P^2

$$
P^2 \approx
\begin{bmatrix}
0.33 & 0.17 & 0.17 & 0.17 & 0.17 & 0   & 0   \\
0.11 & 0.39 & 0.17 & 0   & 0.22 & 0.11 & 0   \\
0.11 & 0.17 & 0.39 & 0.22 & 0   & 0   & 0.11 \\
0.11 & 0   & 0.22 & 0.39 & 0   & 0   & 0.28 \\
0.11 & 0.22 & 0   & 0   & 0.39 & 0.28 & 0   \\
0   & 0.17 & 0   & 0   & 0.42 & 0.42 & 0   \\
0   & 0   & 0.17 & 0.42 & 0   & 0   & 0.42 \\
\end{bmatrix}
$$

---

### (6) 평균 확률 (T=2)

avg=P+P22\text{avg} = \frac{P + P^2}{2}

예:

- (1,4) 직접 연결은 없음 (A14=0A_{14}=0)
- 그러나 avg[1,4] = 0.083 → 랜덤 워크에서 자주 만나므로 잠재적 링크 후보로 점수 상승

---

## 5.5 정리

- 행렬 분해 기반 링크 예측은 인접 행렬을 임베딩 내적으로 근사
- DeepWalk류 기법은 단순 인접행렬 대신 랜덤 워크 기반 공존 확률 행렬 MM을 사용
- 실제 학습은

min⁡Z∑i,j(Mij−ziTzj)2\min_{Z} \sum_{i,j} (M_{ij} - z_i^T z_j)^2

형태로 진행

- **효과**: 직접 연결되지 않아도 랜덤 워크에서 자주 동반되는 노드 쌍을 연결 후보로 잡아낼 수 있음

- 이번 장에서 다루는 대표 GNN 아키텍처:
    1. **그래프 오토인코더(GAE, Graph Autoencoder)**
    2. **변분 그래프 오토인코더(VGAE, Variational Graph Autoencoder)**

---

## 6. 그래프 오토인코더 (GAE)

### (1) 구조

- **인코더(Encoder)**
    - GCN 레이어로 구성
    - 노드 임베딩 ZZ 계산:
        
        Z=GCN(X,A)Z = GCN(X, A)
        
- **디코더(Decoder)**
    - 행렬 분해 & 시그모이드(sigmoid) 함수 사용
    - 인접 행렬 예측:
        
        A^=σ(ZZT)\hat{A} = \sigma(ZZ^T)
        

### (2) 손실 함수

- **바이너리 크로스 엔트로피 (Binary Cross Entropy, BCE)**
    
    LBCE=−∑i,j∈VAijlog⁡(A^ij)+(1−Aij)log⁡(1−A^ij)L_{BCE} = -\sum_{i,j \in V} A_{ij} \log (\hat{A}_{ij}) + (1-A_{ij}) \log (1-\hat{A}_{ij})
    
- 인접 행렬의 각 요소를 0~1 확률로 예측하고, 실제와의 차이를 최소화.

---

## 7. 변분 그래프 오토인코더 (VGAE)

### (1) 차별점

- GAE와 달리, 임베딩을 **확정적(deterministic)** 으로 학습하지 않고,
    
    확률 분포에서 **샘플링**을 통해 생성.
    
- 따라서 불확실성을 반영할 수 있음.

### (2) 구조

- **인코더**
    - GCN 기반
    - 잠재 변수의 평균과 분산을 학습
        
        $$
        μi=GCNμ(X,A),σi2=GCNσ(X,A)\mu_i = GCN_\mu(X, A), \quad \sigma^2_i = GCN_\sigma(X, A)
        $$
        
- **디코더**
    - 샘플링된 잠재 벡터 zz를 사용하여
        
        A^=σ(ZZT)\hat{A} = \sigma(ZZ^T)
        

### (3) 손실 함수

- BCE 손실에 더해 **쿨백–라이블러 발산 (KL Divergence)** 포함:
    
    LELBO=LBCE−KL[q(Z∣X,A)∥p(Z)]L_{ELBO} = L_{BCE} - KL[q(Z|X,A) \| p(Z)]
    
- ELBO(Evidence Lower Bound) 최적화 방식 사용.

---

## 8. VGAE 평가 지표

- **AUROC (Area Under the ROC Curve)**
- **평균 정밀도 (AP, Average Precision)**

→ 링크 예측 성능 평가 시 사용됨.

---

## 9. VGAE 구현 시 특징

- **랜덤하게 링크를 제거**하여 학습 데이터로 사용
- PyTorch Geometric 등에서 VGAE 클래스로 구현 가능
- 처음부터 VGAE를 직접 구현하지 않고 제공된 클래스 사용 가능
- 

| 구분 | 전통적 오토인코더 (Autoencoder) | 그래프 오토인코더 (GAE) | 변분 그래프 오토인코더 (VGAE) |
| --- | --- | --- | --- |
| **입력** | 일반 벡터 데이터 (예: 이미지, 텍스트 특징) | 그래프 데이터 (특징 행렬 XX, 인접 행렬 AA) | 그래프 데이터 (특징 행렬 XX, 인접 행렬 AA) |
| **인코더** | 다층 퍼셉트론(MLP) 등 신경망 | GCN(GCN 레이어 사용) | GCN(평균 μ\mu, 분산 σ2\sigma^2 모두 출력) |
| **잠재 공간 표현** | 고정 벡터 zz | 고정 벡터 zuz_u (노드 임베딩) | 확률 분포 N(μ,σ2)\mathcal{N}(\mu, \sigma^2)에서 샘플링한 zuz_u |
| **디코더** | MLP, 시그모이드, 재구성 네트워크 | 내적 + 시그모이드: \hat{A} = \sigma(ZZ^T) | 내적 + 시그모이드: A^=σ(ZZT)\hat{A} = \sigma(ZZ^T) |
| **목표** | 입력 xx와 복원 x^\hat{x}의 차이 최소화 | 인접 행렬 AA와 예측 A^\hat{A} 차이 최소화 | 인접 행렬 AA와 예측 A^\hat{A} 차이 + KL 발산 최소화 |
| **손실 함수** | MSE, BCE 등 | Binary Cross Entropy (BCE) | Evidence Lower Bound (ELBO): (L_{BCE} - KL[q(Z |
| **불확실성 반영** | 없음 | 없음 | 있음 (분포에서 샘플링) |
| **장점** | 단순, 직관적 | 그래프 구조 반영, 효율적 | 그래프 구조 반영 + 불확실성 추론 가능 |
| **단점** | 그래프 구조 반영 불가 | 확정적 표현만 학습 → 불확실성 무시 | 계산 복잡도 증가, 구현 난이도↑ |
